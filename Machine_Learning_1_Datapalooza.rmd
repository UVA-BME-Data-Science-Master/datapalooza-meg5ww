---
title: "Machine Learning 1, Datapalooza"
author: "Monika Grabowska"
date: "11/8/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Today we will explore an interesting dataset and compare the performance of different machine learning algorithms at classifying human activity using data collected by smartphones.  This is a slightly trickier dataset than most introductory datasets, as the outcome variable (the particular type of activity) is multi-class rather than binary.  This is data from a Kaggle competition.
Let's get started!  First we will install the R packages we will need in order to run our algorithms.  You only need to install packages once, unless you update your version of R, in which ase you will need to do it again.

Now we will "library" those packages.  This needs to be done every time you reopen R, and is simply good practice to do so at the beginning of your code.

```{r}
library(randomForest)
library(gmodels)
library(neuralnet)
library(RSNNS)
library(Rcpp)
library(lattice)
library(ggplot2)
library(caret)
library(knitr)
``` 

Before we begin, we need to set our working directory and load the data.

```{r}
setwd("/Users/monikagrabowska/Documents")
ad <- read.csv("activities_data.csv")
#head(ad)
ad <- ad[,-1]
head(ad)
tail(ad)
## Let's explore a bit:
summary(ad$label) # label is our outcome var

## Clearly R is assuming that the outcome variable, label, is numeric.  ## ## Should it be?  If not, what should it be?
ad$label <- as.factor(ad$label)
```

Before we can create any model, we need to partition the data into training and test sets -- with the training, we will build the model, and with the test set we will evaluate our performance (i.e) judge how successfully we were able to classify the various types of human activity.  

```{r}
## Coerce the ad object to be a dataframe.
ad <- as.data.frame(ad, row.names = NULL)

## Create training and test sets.
index <- sample(nrow(ad), 7500) #randomly pick (w/out replacement) 7500 rows - why? 
train_set <- ad[index,] #index becomes rows for train set (want 75-80% of the data in training set)
test_set <- ad[-index,] #everything not in index becomes rows for test set
summary(test_set$label) #proof that have randomized things (i.e. sanity check)
```

This is a great deal of data for our models to process on a standard PC or laptop, so let's make smaller subsets for the purposes of practicing.

```{r}
practice_train_set <- train_set[1:3500,] #train set already randomized so don't need to do again
practice_test_set <- test_set[1:1500,]
```

Our first model: k-Nearest Neighbor
Let's talk about the underlying assumptions of this model, then create one together.

k-Nearest Neighbor discussion - our closest in feature space (if close in feature to another example with a label on it), can stick same label on new test point
KNN useful when data is labeled, data is noise free, dataset is small (b/c lazy learner - not actually training and building something/doesn't learn discriminative function from dataset)
Don't use even number for k - bc could have 4 red squares and 4 red triangles, need a tie breaker
k-value is not a parameter (not tunable) but a hyperparameter

Synthetic minority oversampling - way of dealing with not evenly distributed data - basically creating fake data

```{r}
set.seed(123) #way of randomizing, allows you to recapitulate that randomization
model_knn <- train(label~ ., data = practice_train_set, 
                       method = "knn", 
                       tuneLength = 5)
# label~. means label is dependent on all features (563)
#tuneLength = 1 - here the k that is used is 5
#tuneLength will pick best k? (runs n number of times with diff k's)

## Make predictions based on this model

p_knn <- predict(model_knn,practice_test_set)

## Check performance:

confusionMatrix(p_knn,practice_test_set$label)
#model_knn
```

(look over this tbh)
Selectivity: how many cases did I catch? 
Specificity: of cases that I caught, how many ex. case 4?

In the confusion matrix, want as large numbers as possible in diag. matrix

Our second model: Random Forest

Thing that discriminates most at top of decision tree
Random forest - putting a bunch of trees together, but want to save time - using randomly chosen subset (chosen with replacement - aka bootstrapped - using less pts to train the tree) - only consider a subset of the attributes of features (typically use sqrt number of features)
In random forest, make a lot of trees - some of quiter nodes get to speak

```{r}
model_rf <- randomForest(label ~ ., practice_train_set)
p_rf <- predict(model_rf,practice_test_set)
confusionMatrix(p_rf,practice_test_set$label)
```

Curse of dimensionality

You've probably noticed that we have a tremendously large set of features; it is difficult to believe that all of them are highly important for classification.  Let's perform some feature selection.

Cool functions in R - variable importance - justification for choosing reduced set of features - shows us in random forest, what were most important discriminating features (horrible names - ex. min gravity accelerometer in X - weighted by number of observations that traversed that particular node)

```{r}
v <- varImp(model_rf)
varImpPlot(model_rf)
```

I will use an arbitrary threshold of 5 then 10 "MeanDecreaseGiniIndex" to choose a strong subset of features. -- just taking top several values and working from there, making new test and train sets 

```{r}
fs <- which(v$Overall>=20)
fs <- fs+1
fs <- c(1,fs)
train_set <- ad[index,fs]
test_set <- ad[-index,fs] #now dimensions of test set 29 features 

#evaluate performance with reduced feature set - performance still good though (very helpful for knn - knn can get distracted by less important features - if use these features with knn will get a lot better results)
model_rf <- randomForest(label ~ ., train_set)
p_rf <- predict(model_rf,test_set)
confusionMatrix(p_rf,test_set$label)
```